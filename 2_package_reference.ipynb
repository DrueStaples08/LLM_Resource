{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT:\n",
    "    - Architecture:\n",
    "\n",
    "        BERT is based off the transformer archictecture but only includes the encoder part (words to embeddings) and not the decoder (embeddings to words). The layers include the following:\n",
    "\n",
    "            - Embedding Layer: Converts tokens to embeddings\n",
    "            - Multi-Head Attention Layer: Determines the attention scores and attention weights for each token utilizing Query, Key, and Value Matrices.\n",
    "            - Feed Forward Neural Network: Extracts relevancy amongst the sum of weighted embeddings. The will output a softmax layer of all the probabilities to a fixed output size. For sentiment analysis, the final layer will produce 2 nodes each with the logit probability for positive and negative sentiment.\n",
    "\n",
    "            - Concat all the matrices from Query, Key, and Value together\n",
    "\n",
    "            - Feed them all into another Feed Forward Neural Network\n",
    "\n",
    "\n",
    "\n",
    "    - Tokenization (STEP 1):\n",
    "\n",
    "        BERT uses a Tokenization technique called WordPiece which helps tokenize a sequence into words and subwords.\n",
    "\n",
    "    - Bidirectional Embeddings (STEP 2):\n",
    "        \n",
    "        It is also bidirectional meaning it can gather richer semantic word relationships when processing from both left-to-right and/or right-to-left in a sentence when generating the word representations (embeddings). Therefore a token is compared with all the surrounding tokens to create the word embedding. For text generation or casual language model (ie. predicting a series of text or a single word to complete the prompt), only the words to the left of the new token matters. \n",
    "\n",
    "    - Pre-training (STEP 3A):\n",
    "\n",
    "        BERT is then pre-trained towards a specific task on a large corpus of text, which can learn the sentiments of tokens and its relavancy, the semantic similarity, and the contextual meaning. All of these traits can be combined together to form the embeddings for each token, which are known as Query, Key, and Value Matrices. \n",
    "        \n",
    "        Note: The pre-training should already be completed before fine-tuning a model, that of which was pre-trained by other developer(s).\n",
    "\n",
    "        - Extracting Query, Key, and Value Matrices: \n",
    "            The matrices are initialized from the input embedding which then serve as learnable parameters for the model since the embeddings pass through a repeated 2 layer process (multi-head attention, feed forward network) n times.\n",
    "\n",
    "              \n",
    "            e.g.   \"I liked the movie, but hated the score.\"\n",
    "\n",
    "            - Query: Specifies the target of what to search for based on the specific LLM task e.g. NER, sentiment analysis, text generation, MLM, etc. For example if BERT is training on a movie review dataset for sentiment analysis, the query matrix Q might contain a 2xn matrix where n is the number of token embeddings (i.e 4 tokens means to have 4 columns each representing a word embedding). The two rows represent positive and negative sentiments (Q_pos, Q_neg) which are the targets. Basically, the sentiement of each word is taken into account. The model then uses the matrix to identify specific words (tokens) for conveying a positive or negative sentiment \n",
    "\n",
    "                query_token_sentiment_dataset = [\n",
    "                    [Q_pos(I), Q_pos(liked), Q_pos(the), Q_pos(movie), Q_pos(but), Q_pos(hated), Q_pos(the), Q_pos(score), Q_pos(.)],\n",
    "                    [Q_neg(I), Q_neg(liked), Q_neg(the), Q_neg(movie), Q_neg(but), Q_neg(hated), Q_neg(the), Q_neg(score), Q_neg(.)]\n",
    "                    ]\n",
    "\n",
    "                Note: Q_neg can be written as (1 - Q_pos)\n",
    "\n",
    "            - Key: Relevant information regarding the other tokens in a input sequence such as the following: \n",
    "            \n",
    "                - Relevance towards sentiment analysis: determining what words are parts-of-speech: nouns, verbs, adjectives, etc. as they have a big impact on the sentiment based on dataset of words and word_type\n",
    "                \n",
    "\n",
    "                key_sentiment_relevance_dataset = {\n",
    "                    \"I\": \"noun\",\n",
    "                    \"liked\": \"verb\",\n",
    "                    \"the\": \"\",\n",
    "                    \"movie\": \"noun\",\n",
    "                    \"but\": \"\",\n",
    "                    \"hated\", \"verb\",\n",
    "                    \"score\": \"noun\"\n",
    "                    \".\": \"\"\n",
    "                }\n",
    "\n",
    "                - Contextual information: An adjustable-sized window of the whole sequence that slides over, so each token is taken into account when assisting with the K matrix. Therefore, each token may have neighboring words before and/or after. The combined sentiment of all the words in each window represents the contextual value for that particular token. Additionally, the K matrix will be different for each head in the Multi-Head because the vector V (see Value Vector section below) that contains the tokens for the sequence or prompt are REORDERED and will have a LARGER OR SMALLER window size. Therefore each token will have different neighboring words for each K matrix in each attention head. \n",
    "\n",
    "                e.g. for simplicity sake, let's keep the order of the sequence the same for this Attention Head. With a window size of 1, a single token before and after the main token is taken into account. \n",
    "\n",
    "                key_contextual_info_dataset_ = [\n",
    "                    [\"<PAD>\", \"I\", \"like], # I\n",
    "                    [\"I\", \"like\", \"the\"], # like\n",
    "                    [\"like\", \"the\", \"movie\"], # the \n",
    "                    [\"the\", \"movie\", \"but\"], # movie\n",
    "                    [\"movie\", \"but\", \"hated\"], # but\n",
    "                    [\"but\", \"hated\", \"the\"], # hated\n",
    "                    [\"hated\", \"the\", \"score\"], # the  \n",
    "                    [\"the\", \"score\", \".\"], # score\n",
    "                    [\"score\", \".\", \"<PAD>\"] # .\n",
    "                ]\n",
    "\n",
    "                Each word is represented by its respective window of tokens, which is presented as a sentiment value for each window\n",
    "                \n",
    "                key_contextual_info_sentiment_dataset_ = [\n",
    "                    [.67], # I: [\"<PAD>\", \"I\", \"like]\n",
    "                    [.74], # like: [\"I\", \"like\", \"the\"]\n",
    "                    [.82], # the: [\"like\", \"the\", \"movie\"]\n",
    "                    [.55], # movie:[\"the\", \"movie\", \"but\"]\n",
    "                    [-.94], # but: [\"movie\", \"but\", \"hated\"]\n",
    "                    [-.79], # hated: [\"but\", \"hated\", \"the\"]\n",
    "                    [-.45], # the: [\"hated\", \"the\", \"score\"]\n",
    "                    [.27], # score: [\"the\", \"score\", \".\"]\n",
    "                    [.09] # .: [\"score\", \".\", \"<PAD>\"]\n",
    "                ]\n",
    "\n",
    "                - Semantic meaning: Sentiments for each token based on pre-built sentiment lexicon dataset containing BOTH WORDS AND THEIR SENTIMENTS (-1->1).\n",
    "\n",
    "                key_semantic_meaning_dataset = {\n",
    "                    \"I\": .50,\n",
    "                    \"liked\": .82,\n",
    "                    \"the\": .46,\n",
    "                    \"movie\": .43,\n",
    "                    \"but\": -.56,\n",
    "                    \"hated\": -.97,\n",
    "                    \"score\": .56,\n",
    "                    \".\": .00\n",
    "                }\n",
    "\n",
    "            - Value: The actual information to be attended so the vector V contains the embeddings for each token, but in DIFFERENT ORDER FOR EACH ATTENTION HEAD. For further details, see the Multi-Head section below. \n",
    "\n",
    "                value_actual_token_embeddings = [v1, v2, v3, v4, v5, v6, v7, v8, v9]\n",
    "\n",
    "                where vn represents the embedding of each token. \n",
    "\n",
    "\n",
    "        - Self Attention (Attention Scores):\n",
    "            - BERT's mechanism computes the attention scores for each tokens' information about the other tokens in the sequence. The tokens' scores represents how important or how much the word contributes to the surrounding words. \n",
    "            - This is done by computing the dot product between the Query and Key embeddings within the Q and K matrices scaled by the square root of the dimensionality of the key vectors. Then softmax function is then used to find the attention scores i.e. the highest logit probalities for each token are chosen.\n",
    "\n",
    "        - Attention Weights:\n",
    "        Find the weighted sum between the value vector (column vector) and the attention score matrix (per row) via vector matrix multiplication, to yield the attention output for each word. \n",
    "\n",
    "            e.g. \n",
    "\n",
    "                input = 'I love learning.'\n",
    "                tokens = ['I', 'love', 'learning', '.']\n",
    "\n",
    "                # Value vector with vN representing each token as an embedding\n",
    "                value_vector = [v1, v2, v3, v4]\n",
    "\n",
    "                I = v1\n",
    "                love = v2\n",
    "                learning = v3\n",
    "                . = v4\n",
    "                \n",
    "                # attention_scores = dot_product_per_sample_matrix(Key Matrix, Query Matrix) (******Matrices have been computed during the original training process through gradient descent and backpropagation) \n",
    "                attention_scores = [ \n",
    "                [.8, .2, .06, .05],   # I\n",
    "                [.3, .7, .03, .04],  # love\n",
    "                [.1, .02, .8, .04], # learning\n",
    "                [.01, .03, .05, .9] # .\n",
    "                ].T\n",
    "\n",
    "                attention weights = sum(matmul(value_vector, attention_scores))\n",
    "\n",
    "                i.e.\n",
    "                                        I           love      learning        .\n",
    "                attention_weight_I = (v1 * .8) + (v2 * .2) + (v3 * .06) + (v4 * .05)\n",
    "                attention_weight_love = (v1 * .3) + (v2 * .7) + (v3 * .03) + (v4 * .04)\n",
    "                attention_weight_learning = (v1 * .1) + (v2 * .02) + (v3 * .8) + (v4 * .04)\n",
    "                attention_weight_. = (v1 * .01) + (v2 * .03) + (v3 * .05) + (v4 * .9)\n",
    "\n",
    "\n",
    "        - Multi-head Attention:\n",
    "        Multi-head Attention Layer is applied by computing different Query, Key, and Value Matrices in parallel, which are all then concatenated with each other then linearly transformed to output the attention scores in that MHA Layer before passing on to the Feed Forward NN. \n",
    "        \n",
    "        Note: Based on the input, each head will have a Value Matrix of embeddings which consist of reordered input token embeddings (see below). Therefore the Key and Query matrices will contain different embedding values for tokens. \n",
    "\n",
    "            e.g. \n",
    "\n",
    "                input = 'I love learning.'\n",
    "                tokens = ['I', 'love', 'learning', '.']\n",
    "\n",
    "                # Assume vN are embeddings for EACH word or character above.\n",
    "\n",
    "                # For Attention Head 1\n",
    "                value_vector_1 = [v1, v2, v3, v4]\n",
    "\n",
    "                # For Attention Head 2\n",
    "                value_vector_2 = [v3, v4, v1, v2]\n",
    "\n",
    "                # For Attention Head 3\n",
    "                value_vector_3 = [v4, v3, v2, v1]\n",
    "\n",
    "\n",
    "    - Feed Forward Neural Network (STEP 3B):\n",
    "     Feed the concatenated attention outputs from each head of the Multi-head Attention mechanism into a feedforward network which processes and learns other hierarchal features. This is then fed to another attention layer where the process is then repeated. \n",
    "\n",
    "\n",
    "    - Fine-tuning (STEP 4):\n",
    "    After pre-training, BERT's checkpoint can then be finetuned towards a specific downstream task (pretrained BERT model with checkpoint parameters) with little supervised data. The parameters are then updated as the model learns the target task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- DistilBERT (6 layers): \n",
    "\n",
    "    Same as BERT, but its 40% smaller which makes it faster. Another interesting point to make out, is that DistilBERT can retain most (~97%) of the functionality from BERT making it useful all around. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to move LLM into production for NLP tasks?\n",
    "\n",
    "1. Model Selection and Training\n",
    "    1. Choose the right model for the appropriate task\n",
    "    2. Fine-Tune or train model \n",
    "2. Infrastructure setup\n",
    "    1. Choose Cloud Service\n",
    "    2. Ensure the cloud resources are sufficient enough for model workloads\n",
    "3. Model Deployment\n",
    "    1. Containerization of model with Docker, S3/Artifact Registry\n",
    "    2. Further Deploy with Kubernetes and a virtual server (e.g. Compute Engine) for managing containers and scaling application (load balancer and auto-scaling)\n",
    "4. API Development\n",
    "    1. Build a Flask app that calls the docker container image (ml model) for serving model inference\n",
    "    2. Load Balancing: Implement load balancing to evenly distribute incoming requests to multiple instances (pods)\n",
    "5. Monitoring and Logging\n",
    "    1. Monitor model performance, latency (user request delays), and uptime (assurance that network is working for users to make requests). Tools include Prometheus and Grafana\n",
    "    2. Setup logging which will help track and analyze errors and usage patterns. Tools include ELK stack (Elasticsearch, Logstash, Kibana) \n",
    "6. Scaling\n",
    "    1. Horizontal Scaling: Add more instances (vm’s) to handle the number of workloads\n",
    "    2. Vertical Scaling: Increasing the size of GPUs (or CPUs) to handle the size of workloads.\n",
    "7. Security\n",
    "    1. Authentication and Authorization: Implement OAuth2.0 for extra layer of verification to help secure API\n",
    "    2. Data Encryption: Use HTTPS to protect data (whether its being transferred or not)\n",
    "8. Model Optimization\n",
    "    1. Reduce Latency by refactoring pipelines using efficient data structures.\n",
    "    2. Managing Cost via monitoring and optimizing resources that are being billed to company \n",
    "9. Maintenance and Updates:\n",
    "    1. Setup a CI/CD pipeline (GitHub Actions, cloud functions and source repositories) for automated testing and deployment so code (new model versions) runs through a series of tests before being merged to main branch, moved to the dev environment, then to the production environment.\n",
    "    2. Update model with new data to prevent model drift. Model drifts involves two types of drifts: data drift and concept drift. Concept drift is when the relationship between the features and the target change. Data drift is when the features change.\n",
    "\n",
    "Tools and Technologies\n",
    "* Containerization and Orchestration: Docker, Kubernetes\n",
    "* API Development: Flask, FastAPI, Django\n",
    "* Monitoring and Logging: Prometheus, Grafana, ELK Stack\n",
    "* CI/CD: Jenkins, GitHub Actions, GitLab CI\n",
    "* Security: OAuth2.0, HTTPS, JWT\n",
    "\n",
    "Example Workflow\n",
    "1. Development: Train and fine-tune your model locally.\n",
    "2. Containerization: Create a Docker image of your application.\n",
    "3. Deployment: Deploy the Docker image on a Kubernetes cluster.\n",
    "4. API: Expose the model through a RESTful API.\n",
    "5. Monitoring: Set up monitoring and logging.\n",
    "6. Scaling: Implement horizontal and vertical scaling based on demand.\n",
    "7. Maintenance: Use CI/CD pipelines for automated testing and deployment.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLM for Named Entity Recognition (NER) using HuggingFace transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Classification is the task of identifying classes for particular tokens like person (names), places (locations), or company names (organization).  \n",
    "\n",
    "Example below shows how to train Distil-BERT on the WNUT 17 datset, save the model to model hub, then perform inference with the newly saved fined-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to HF (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use \n",
    "\n",
    "notebook_login()\n",
    "\n",
    "OR \n",
    "\n",
    "run `huggingface-cli login` in terminal\n",
    "\n",
    "i.e. Type \"huggingface-cli login\" in terminal to login to HuggingFace. This allows you to push your saved models to the HuggingFace database to share openly to all users and to load in for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wnut_17\")\n",
    "dataset\n",
    "''' \n",
    "B- represents the beginning of an entity e.g \"B-Paris\" for Paris France \n",
    "I- represents an element associated with the B- tage e.g. \"I-France\" for Paris France\n",
    "O- no entity was identified\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate labels for each token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k:v for k,v in enumerate(dataset['train'].features['ner_tags'].feature.names)}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "label2id\n",
    "# Tokens i.e. list of words for each sequence\n",
    "# dataset['train']['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Dataset Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Below is just an example but to preprocess an entire dataset, then use a map function which includes the preprocessing function\n",
    " \n",
    "example_input_info = tokenizer(dataset['train']['tokens'][0], max_length=50, truncation=True, is_split_into_words=True)\n",
    "# tokenizer.decode(example_input_info['input_ids'])\n",
    "example_tokens = tokenizer.convert_ids_to_tokens(example_input_info['input_ids'])\n",
    "example_tokens\n",
    "example_input_info['input_ids'][:10], example_tokens[:10]\n",
    "\n",
    "example_input_info.word_ids(batch_index=0)\n",
    "for idx, val in enumerate(example_input_info['input_ids']):\n",
    "    print(idx, val)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-align token labels with the word ids then ner labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter \"is_split_into_words\" creates a mismatch in labels (ner tags).\n",
    "Therefore re-align token labels with the word ids then ner labels. Word ids\n",
    "are ids where each subword has the same id. From there, only the first \n",
    "unique label id stays the same and the rest convert to -100. Additionally,\n",
    "special tokens don't have word ids so they are represented as None values. Afterwards,\n",
    "they are converted to -100 values in the ner tags outputs.\n",
    "\n",
    "e.g. \n",
    "sequence: 'playground'\n",
    "tokens: (with is_split_into_words=True): ['[CLS]', 'play', 'ground', '[SEP]']\n",
    "word ids: [None, 1, 1, None]\n",
    "token ids: [-100, 5, -100, -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_dataset(sequences: str):\n",
    "    # Tokenize sequence of sentences to words with special characters and subwords included\n",
    "    # e.g. \"playground\" -> [\"CLS\", \"play\", \"ground\", \"SEP\"]\n",
    "    # AND ....\n",
    "    # Convert each sequence into its respective token ids\n",
    "    # e.g. [\"CLS\", \"play\", \"ground\", \"SEP\"] -> [101, 344, 9948, 102]\n",
    "    tokenized_info = tokenizer(sequences['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_new_ner_tags = []\n",
    "\n",
    "    ner_tags = sequences['ner_tags']\n",
    "\n",
    "    for ner_idx, ner_tag in enumerate(ner_tags):\n",
    "        # Convert each token id to its associated word id\n",
    "        # A word id represents a unique word or group of subwords\n",
    "        # that represents the label or ner tag it's associtated with.\n",
    "        # For the example below, both \"play\" and \"ground\" receive the same\n",
    "        # id since \"ground\" is the *inner subword of \"play\" (which is the \n",
    "        # beginning of the subword). \n",
    "        # [101, 344, 9948, 102] -> [None, 2, 2, None]\n",
    "        word_ids = tokenized_info.word_ids(batch_index=ner_idx)\n",
    "        prev_idx = None\n",
    "        single_new_ner_tag = []\n",
    "\n",
    "        # Swap the None values and repetive subwords for the value -100 and keep the remaining word ids\n",
    "        # [None, 2, 2, None] -> [-100, 2, -100, -100] \n",
    "        # Another example -> [None, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 6, None] -> [-100, 1, 2, 3, 4, 5, 6, -100]\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                single_new_ner_tag.append(-100)\n",
    "            elif prev_idx != word_idx:\n",
    "                single_new_ner_tag.append(ner_tag[word_idx])\n",
    "            else:\n",
    "                single_new_ner_tag.append(-100)\n",
    "            prev_idx = word_idx\n",
    "        all_new_ner_tags.append(single_new_ner_tag)\n",
    "\n",
    "    # Add the labels that was just created to the 'tokenized_info' DatasetDict    \n",
    "    tokenized_info['labels'] = all_new_ner_tags\n",
    "    return tokenized_info\n",
    "\n",
    "            \n",
    "new_data = data.map(create_new_input_ids_for_new_token_list, batched=True)\n",
    "new_data\n",
    "new_data['train']['labels'][4]\n",
    "        \n",
    "        # Option 2: save results in-place\n",
    "    #     current_word_id = None\n",
    "    #     for word_idx, word_id in enumerate(single_sequence_ids):\n",
    "    #         if word_id is None or word_id == current_word_id:\n",
    "    #             single_sequence_ids[word_idx] = -100\n",
    "    #             current_word_id = single_sequence_ids[word_idx]\n",
    "    # input_info['labels'] = input_ids\n",
    "    # return input_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT using batched=True hyperparameter, outputs a \"IndexError: list index out of range\"\n",
    "tokenized_dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "\n",
    "tokenized_dataset\n",
    "tokenized_dataset['train']['labels']\n",
    "for i in tokenized_dataset['train']['labels']:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Dynamic Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above the len of each sequence is mismatched. \n",
    "Each sequence greater than the max length limit was truncated to the max length, however sequences \n",
    "with a length less than the maximum will still be short. Therefore padding still needs to take \n",
    "place. \n",
    "Q.) But if I add the hyperparameter padding=True or padding=max_length or max_length=50 to the \n",
    "tokenizer object, then why do I still have mismatched lengths? \n",
    "A.) The subwords for each word are computed AFTER the initial padding. \n",
    "Meaning that no matter what, there will be a mismatch between tokens and word ids. To solve this \n",
    "problem, each sequence must be padded to be the same length. \n",
    "There are two obvious ways to do this: \n",
    "1. pad the list of sequences as a whole or \n",
    "2. pad a batch of sequences. The first option is cumbersome and can be computationally expensive \n",
    "especially with a large amount of data. Essentially, it involes just detmining the sequence with \n",
    "the max length and then padding the remaining sequences with zeros (left and/or right padding) \n",
    "to reach the max length. The better option would be to extract a batch of data \n",
    "(n samples of whole dataset) and pad based off the sequence in the batch with the max length. \n",
    "This means batches of data will contain the same sequence length, but can be different size when compared to other batches\n",
    "\n",
    "e.g. dynamic padding with batches of 2, truncation=True and max_length=7\n",
    "\n",
    "batch 1\n",
    "[16,28,43]\n",
    "[86,25,47,54,765,3,45,23,66,75,89,43]\n",
    "[76,55,987,4,1,22,33,16,54]\n",
    "\n",
    "dynamic padding: batch 1 \n",
    "[16,28,43,0,0,0,0]\n",
    "[86,25,47,54,765,3,45]\n",
    "[76,55,987,4,1,22,33]\n",
    "\n",
    "batch 2\n",
    "[245,98,8,74,14,75,3]\n",
    "[6,30,5,7]\n",
    "[66,88,31]\n",
    "\n",
    "dynamic padding: batch 2\n",
    "[245,98,8,74,14,75,3]\n",
    "[6,30,5,7,0,0,0]\n",
    "[66,88,31,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqeval is an easy-to-implement library for model evaluation that includes various metrics, like precison, recall, accuracy, f1 score, etc.\n",
    "\n",
    "# Create an evaluation function which will later be inputed as a one of the training arguements for the Trainer class\n",
    "# Therefore the evaluation function will kick off as the model is training.\n",
    "# The seqeval framework includes several metrics to use like precision, recall, f1 score, and accuracy\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for evaluation metrics\n",
    "# Use mapping dict for ner_labels\n",
    "# e.g. id2label and label2id\n",
    "# Create compute metrics function which will return when training is complete.\n",
    "# It will be be instantiated inside the TrainerArguements which can hold \n",
    "# range of hyperparamters to tune. \n",
    "# nx13 size input\n",
    "def compute_metrics(predictions_for_each_ner_tag):\n",
    "    probs, labels = predictions_for_each_ner_tag\n",
    "    print('labels', labels)\n",
    "    predictions = np.argmax(probs, axis=2)\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    # true_labels = [\n",
    "    #     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    #     for prediction, label in zip(predictions, labels)\n",
    "    # ]\n",
    "    print('true labels', true_labels)\n",
    "    true_predictions = [[id2label[p] for (p,l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    # true_predictions = [\n",
    "    #         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    #         for prediction, label in zip(predictions, labels)\n",
    "    #     ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    # result = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Trainer is attempting to log a value of \"{'precision': 0.14285714285714285, 'recall': 0.045454545454545456, 'f1': 0.06896551724137931, 'number': 66}\" of type <class 'dict'> for key \"eval/corporation\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
    "    print('RESULTS------', results)\n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "\n",
    "    }\n",
    "    # return {\n",
    "    #         \"precision\": results[\"overall_precision\"],\n",
    "    #         \"recall\": results[\"overall_recall\"],\n",
    "    #         \"f1\": results[\"overall_f1\"],\n",
    "    #         \"accuracy\": results[\"overall_accuracy\"],\n",
    "    #     }\n",
    "\n",
    "    # print('TP', true_predictions)\n",
    "    # print('TL', true_labels)\n",
    "    # return true_labels\n",
    "\n",
    "    # return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model or use pre-trained one to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instatiate a TrainingArgs object with a set of arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(output_dir=\"./ner_model_wnut17\",\n",
    "                                  push_to_hub=True,\n",
    "                                  learning_rate=2.5e-5,\n",
    "                                  weight_decay=0.05,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  num_train_epochs=3)\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Instantiate Trainer object\n",
    "\n",
    "# Before Training, quickly create two dictionaries: id2label and label2id\n",
    "# This will be helpful as reference when outputting predictions. \n",
    "\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_dataset['train'],\n",
    "                  eval_dataset=tokenized_dataset['test'],\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  args=training_args\n",
    "                  )\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference after pushing model to Huggingface Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference Method 1: use pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load model to HF model hub\n",
    "sequence = \"The man works in Indiana for Prolific Inc.\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "pipe = pipeline(task=\"ner\", model=\"dstaples08/ner_model_wnut17\")\n",
    "pipe\n",
    "pipe(sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infererence Method 2: Replicate the results of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dstaples08/ner_model_wnut17\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dstaples08/ner_model_wnut17\")\n",
    "tokenizer\n",
    "model\n",
    "seq_vector = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "tokenized_info = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokenized_info\n",
    "seq_vector\n",
    "with torch.no_grad():\n",
    "    model_info = model(**tokenized_info)\n",
    "# logits = model(tokenized_info[\"input_ids\"])[\"logits\"]\n",
    "model_info\n",
    "logits = model_info[\"logits\"]\n",
    "logits\n",
    "logits.shape, \n",
    "label_map = model.config.id2label\n",
    "label_map\n",
    "predictions_label_ids = logits.argmax(dim=2).tolist()\n",
    "predictions_label_ids\n",
    "sequence\n",
    "predictions = [label_map[i] for i in predictions_label_ids[0]]\n",
    "predictions\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

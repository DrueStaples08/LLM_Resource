{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLM for Sentiment Analysiss using HuggingFace transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification is a patten-based approach of classifiying labels based on a set of categories. A popular form of Text Classification is Sentiment Analysis to categorize text into sentiments i.e. positive, neutral, and negative. This can be applied to applications like movie reviews, political posts, or stock trading.\n",
    "\n",
    "The goal is to use DistilBERT on the imdb dataset to predict whether a movie review is either positive or negative.\n",
    "\n",
    "\n",
    "Note: To Fine-Tune a model, it must adhere to the pre-trained model's target task.\n",
    "\n",
    "Text Classification is a patten-based approach of classifiying labels based on a set of categories. A popular form of Text Classification is Sentiment Analysis to categorize text into sentiments i.e. positive, neutral, and negative. This can be applied to applications like movie reviews, political posts, or stock trading.\n",
    "\n",
    "The goal is to use DistilBERT on the imdb dataset to predict whether a movie review is either positive or negative\n",
    "\n",
    "#### Install/import packages + login to Huggingface Account for pushing model capabilities\n",
    "# pip install transformers datasets evaluate accelerate\n",
    "'''\n",
    "By logging in to the HF Hub, you can upload your fined-tuned model and tokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "Problem with loggin in to HF Hub\n",
    "ImportError: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\n",
    "\n",
    "\n",
    "Solution:\n",
    "open up terminal and run `huggingface-cli login`\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install/import packages + login to Huggingface Account for pushing model capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBy logging in to the HF Hub, you can upload your fined-tuned model and tokenizer\\nfrom huggingface_hub import notebook_login\\n\\nProblem with loggin in to HF Hub\\nImportError: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\\n\\n\\nSolution:\\nopen up terminal and run `huggingface-cli login`\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install transformers datasets evaluate accelerate\n",
    "'''\n",
    "By logging in to the HF Hub, you can upload your fined-tuned model and tokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "Problem with loggin in to HF Hub\n",
    "ImportError: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\n",
    "\n",
    "\n",
    "Solution:\n",
    "open up terminal and run `huggingface-cli login`\n",
    "'''\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druestaples/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load Dataset\n",
    "data = load_dataset(\"imdb\")\n",
    "data\n",
    "data['unsupervised']['text'][:10]\n",
    "data['train']['text'][:10]\n",
    "data['train']['label'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:03<00:00, 7162.35 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:03<00:00, 7506.40 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:07<00:00, 7094.95 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[50, 50, 50]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a map() function which inputs a dictionary then applies a function to each row\n",
    "\n",
    "def preprocess_dataset(sequence):\n",
    "    return tokenizer(sequence['text'], truncation=True, padding=True, max_length=50, add_special_tokens=True)\n",
    "    # return tokenizer.encode(sequence['text'], return_tensors=\"pt\", truncation=True)\n",
    "tokenized_data = data.map(preprocess_dataset,batched=True)\n",
    "tokenized_data\n",
    "# tokenized_data['train']['text'][:3]\n",
    "[len(i) for i in tokenized_data['train']['input_ids'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Dynamic Padding \n",
    "# It's more optimized to...\n",
    "# pad a batch of sentences based on the max length of the longest sequence in that particular batch\n",
    "# opposed to padding every sentence in the entire dataset based on the token with the maximum length. \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate addition metrics if needed for evaluation on train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a number of metrics for the model from the \"evaluate\" library.\n",
    "\n",
    "acc_score = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(predicted_probs):\n",
    "    logits, labels = predicted_probs\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return acc_score.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label_2_ids and label_2_ints dictionaries for easy reference when printing out the predictions\n",
    "\n",
    "labels_2_ids = {'POSITIVE': 1, 'NEGATIVE': 0}\n",
    "ids_2_labels = {1: 'POSITIVE', 0: 'NEGATIVE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from HF Model Hub and instatiate the hyperparameters: number of output labels, dictionary of both ids to labels and labels to ids\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, id2label=ids_2_labels, label2id=ids_2_labels)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguements - only one arguement is required \"output_dir\"\n",
    "\n",
    "training_args = TrainingArguments(    \n",
    "    output_dir=\"my_awesome_modelX\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True)\n",
    "    # push_to_hub=True,                          \n",
    "                                  # learning_rate= 15e-5,\n",
    "                                  # weight_decay=.2,\n",
    "                                  # output_dir=\"simple_model\",\n",
    "                                  # per_device_train_batch_size=16,\n",
    "                                  # per_device_eval_batch_size=16,\n",
    "                                  # num_train_epochs=5,\n",
    "                                  # evaluation_strategy=\"epoch\",\n",
    "                                #   load_best_model_at_end=True,\n",
    "                                #   push_to_hub=True\n",
    "\n",
    "                                  #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druestaples/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x179fad880>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Trainer class which can include the model, train/test data, tokenizer, training arguements, data collator, and a metrics function \n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_data['train'],\n",
    "                  eval_dataset=tokenized_data['test'],\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [01:53<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2067, 'grad_norm': 0.07200344651937485, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [02:38<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.249, 'grad_norm': 7.350505352020264, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [03:23<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2426, 'grad_norm': 25.27147674560547, 'learning_rate': 4.2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [04:08<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2375, 'grad_norm': 16.328832626342773, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [04:54<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2468, 'grad_norm': 17.963199615478516, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [05:39<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2517, 'grad_norm': 26.059907913208008, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [06:23<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1634, 'grad_norm': 0.07462353259325027, 'learning_rate': 3.1333333333333334e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [07:07<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-4000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1189, 'grad_norm': 0.4949056804180145, 'learning_rate': 2.8666666666666668e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [23:56<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-4500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1334, 'grad_norm': 0.020918797701597214, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [24:41<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-5000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1277, 'grad_norm': 0.18395085632801056, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [25:26<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-5500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1293, 'grad_norm': 32.170372009277344, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [26:11<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-6000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1383, 'grad_norm': 0.07567379623651505, 'learning_rate': 1.8e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [26:56<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-6500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0823, 'grad_norm': 0.022272340953350067, 'learning_rate': 1.5333333333333334e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [27:42<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-7000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0348, 'grad_norm': 0.002939199097454548, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [28:27<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-7500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.057, 'grad_norm': 16.094919204711914, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [29:13<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-8000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0461, 'grad_norm': 0.010443528182804585, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [29:58<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-8500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0424, 'grad_norm': 0.00723448907956481, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|█████▉    | 1872/3125 [30:43<00:19, 64.70it/s]Checkpoint destination directory tmp_trainer/checkpoint-9000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0378, 'grad_norm': 0.008888768032193184, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 9375/9375 [30:09<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1809.7104, 'train_samples_per_second': 41.443, 'train_steps_per_second': 5.18, 'train_loss': 0.13806326314290365, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.13806326314290365, metrics={'train_runtime': 1809.7104, 'train_samples_per_second': 41.443, 'train_steps_per_second': 5.18, 'train_loss': 0.13806326314290365, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:52<00:00, 18.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9095878005027771,\n",
       " 'eval_accuracy': 0.79608,\n",
       " 'eval_runtime': 172.9386,\n",
       " 'eval_samples_per_second': 144.56,\n",
       " 'eval_steps_per_second': 18.07,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate performance on test set \n",
    "trainer.evaluate(tokenized_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1871/3125 [00:29<00:19, 64.70it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Make predictions \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_set_predictions \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(tokenized_data[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m test_set_predictions\n\u001b[1;32m      4\u001b[0m np\u001b[39m.\u001b[39margmax(test_set_predictions[\u001b[39m0\u001b[39m][:\u001b[39m3\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/transformers/trainer.py:3305\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3302\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3304\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3305\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3306\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[1;32m   3307\u001b[0m )\n\u001b[1;32m   3308\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/transformers/trainer.py:3440\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3434\u001b[0m     inputs_host \u001b[39m=\u001b[39m (\n\u001b[1;32m   3435\u001b[0m         inputs_decode\n\u001b[1;32m   3436\u001b[0m         \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m         \u001b[39melse\u001b[39;00m nested_concat(inputs_host, inputs_decode, padding_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m   3438\u001b[0m     )\n\u001b[1;32m   3439\u001b[0m \u001b[39mif\u001b[39;00m logits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3440\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mpad_across_processes(logits, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, pad_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3441\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3442\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/accelerator.py:2338\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_across_processes\u001b[39m(\u001b[39mself\u001b[39m, tensor, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, pad_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, pad_first\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   2306\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2307\u001b[0m \u001b[39m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[39m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2338\u001b[0m     \u001b[39mreturn\u001b[39;00m pad_across_processes(tensor, dim\u001b[39m=\u001b[39;49mdim, pad_index\u001b[39m=\u001b[39;49mpad_index, pad_first\u001b[39m=\u001b[39;49mpad_first)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/utils/operations.py:414\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[1;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    413\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m         \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    415\u001b[0m     \u001b[39mexcept\u001b[39;00m DistributedOperationException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m         operation \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunction\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mfunction\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/utils/operations.py:681\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    678\u001b[0m     new_tensor[indices] \u001b[39m=\u001b[39m tensor\n\u001b[1;32m    679\u001b[0m     \u001b[39mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 681\u001b[0m \u001b[39mreturn\u001b[39;00m recursively_apply(\n\u001b[1;32m    682\u001b[0m     _pad_across_processes, tensor, error_on_other_type\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dim\u001b[39m=\u001b[39;49mdim, pad_index\u001b[39m=\u001b[39;49mpad_index, pad_first\u001b[39m=\u001b[39;49mpad_first\n\u001b[1;32m    683\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported types (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) passed to `\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`. Only nested list/tuple/dicts of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobjects that are valid for `\u001b[39m\u001b[39m{\u001b[39;00mtest_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` should be passed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/accelerate/utils/operations.py:661\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n\u001b[1;32m    660\u001b[0m \u001b[39m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(tensor\u001b[39m.\u001b[39;49mshape, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)[\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    662\u001b[0m sizes \u001b[39m=\u001b[39m gather(size)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    663\u001b[0m \u001b[39m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "test_set_predictions = trainer.predict(tokenized_data['test'])\n",
    "test_set_predictions\n",
    "np.argmax(test_set_predictions[0][:3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_predictions[0][:3]\n",
    "for l, p in zip(tokenized_data['test']['label'][:10], np.argmax(test_set_predictions[0][:10], axis=1)):\n",
    "    print(ids_2_labels[l], ids_2_labels[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the newly fined-tuned model to HF model hub\n",
    "# Note: Be sure to login to HF via terminal using the command `huggingface-cli login`\n",
    "\n",
    "# Link to all models in profile -> https://huggingface.co/dstaples08\n",
    "# Link to newly created model -> https://huggingface.co/dstaples08/tmp_trainer?text=but+the+acting+brought+the+quality+down+some.+They+should+remove+the+main+cast+otherwise+my+curiousity+for+a+sequel+is+low.\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "After pushing model to hub (make sure to use either Colob or notebook),\n",
    "it is now available for anyone to use for inference or further fine-tuning with just a simple path to the model when defining a pipeline or AutoModel/AutoTokenizer. There are two ways to run inference on a fine-tuned model:\n",
    "\n",
    "1. Use the simple pipeline() function from transformers library\n",
    "\n",
    "2. Replicate the results of the pipeline i.e use AutoTokenizer.from_pretrained(\"dstaples/simple_model\") and AutoModelForSequenceClassification.from_pretrained(\"dstaples/simple_model\")\n",
    "text = \"I really liked the movie, but the acting brought the quality down some. They should remove the main cast otherwise my curiousity for a sequel is low.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use the simple pipeline() function from transformers library\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis_classifier = pipeline(\"sentiment-analysis\", model=\"dstaples08/tmp_trainer\")\n",
    "\n",
    "result = sentiment_analysis_classifier(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Replicate the results of the pipeline i.e use AutoTokenizer.from_pretrained(\"dstaples/simple_model\") and AutoModelForSequenceClassification.from_pretrained(\"dstaples/simple_model\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"dstaples08/tmp_trainer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"dstaples08/tmp_trainer\")\n",
    "tokenizer\n",
    "model\n",
    "input_info = tokenizer(text, return_tensors=\"pt\")\n",
    "input_info\n",
    "input_ids = input_info['input_ids']\n",
    "input_ids.shape\n",
    "with torch.no_grad():\n",
    "    # Same as logits = model(input_ids)\n",
    "    model_info = model(**input_info) \n",
    "model_info\n",
    "logits = model_info['logits']\n",
    "logits\n",
    "ids_2_labels[logits.argmax().item()]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
